{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensorless Drive Diagnosis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the project\n",
    "\n",
    "<b>To build a multi-class classification model with numerical attributes</b>\n",
    "\n",
    "\n",
    "The task is to classify the condition of a synchron motor by using current measurements in the motor. \n",
    "\n",
    "<b>Research Paper </b>\n",
    "\n",
    "[The original paper](https://www.researchgate.net/publication/262698433_Sensorlose_Zustandsuberwachung_an_Synchronmotoren)  is in German language. [Here](https://www.researchgate.net/publication/264273485_Feature_Extraction_and_Reduction_Applied_to_Sensorless_Drive_Diagnosis) is the English version. \n",
    "\n",
    "After the advances in industrial information technology, condition monitoring methods are becoming increasingly important. The phase currents are used at the evaluation of the process data without additional, cost-intensive sensors and the determination the damage status of a syn-characterize chronomotors and the connected components. \n",
    "\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "<b>Dataset used in the analysis: </b> Sensorless Drive Diagnosis dataset (SDD): \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Dataset+for+Sensorless+Drive+Diagnosis \n",
    "\n",
    "The dataset contains information collected from electric current drive signals of a synchronous electric motor. The current signals are measured with a current probe and an oscilloscope on two phases.\n",
    "\n",
    "<b>Raw Data: </b> \n",
    "\n",
    "In the raw data there are a lot of measurements (different speeds, load moments and load forces) and different health conditions (motor ok, failure in the bearings, etc. ) \n",
    "\n",
    "The raw data is timeseries data. However the dataset provided are only the extracted features from the raw data. \n",
    "\n",
    "<b>Features and classes: </b>\n",
    "\n",
    "There are 48 continous predictive features. The target feature contains 11 classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.7.4\n",
    "sklearn.__version__ 0.23.2\n",
    "pandas.__version__ 0.25.1\n",
    "numpy.__version__ 1.17.2\n",
    "matplotlib.__version__ 3.1.1\n",
    "seaborn.__version__ 0.9.0\n",
    "xgboost.__version__ 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.936778Z",
     "start_time": "2021-04-09T04:49:41.697702Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/data-science/sensorless-drive-diagnosis/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.940527Z",
     "start_time": "2021-04-09T04:49:41.604Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# plot pretty figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "# sklearn machine learning \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score, fbeta_score, r2_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# sklearn multi layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.942589Z",
     "start_time": "2021-04-09T04:49:41.609Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions are in a Python script at the same folder. \n",
    "%run -i 'functions.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.946060Z",
     "start_time": "2021-04-09T04:49:41.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is the same as the above cell. No need to run this if you downloaded the GitHub repo. \n",
    "\n",
    "def plot_coefficients(clf):\n",
    "    \"\"\"\n",
    "        The function to plot the coefficients of logistic regression model.\n",
    "  \n",
    "        Parameters:\n",
    "            clf: classifier model   \n",
    "    \"\"\"\n",
    "    \n",
    "    weights_clf = pd.Series(clf.coef_[0], index=scaled_X_train_log.columns.values)\n",
    "    weights_clf.sort_values(inplace=True)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.xticks(rotation=90)\n",
    "    #barplot\n",
    "    features = plt.bar(weights_clf.index, weights_clf.values)\n",
    "    \n",
    "    \n",
    "    \n",
    "def model_evaluation(X_train, X_test, y_train, y_test, y_pred_train, y_pred_test):\n",
    "    \"\"\"\n",
    "        The function to print the metrics of the model evaluation.\n",
    "        Printed metrics: Accuracy scores, \n",
    "                         confusion matrixes, \n",
    "                         and classification reports for train and test sets, \n",
    "  \n",
    "        Parameters: Train, test, and prediction values\n",
    "                    X_train, X_test, \n",
    "                    y_train, y_test, \n",
    "                    y_pred_train, y_pred_test   \n",
    "    \"\"\"\n",
    "\n",
    "    print('MODEL EVALUATION METRICS:\\n',\n",
    "          '-----------------------------------------------------')\n",
    "    print('Train Set Accuracy Score', round(accuracy_score(y_train, y_pred_train), 6))\n",
    "    print('Test Set Accuracy Score', round(accuracy_score(y_test, y_pred_test),6))\n",
    "    print('-----------------------------------------------------\\n')\n",
    "\n",
    "    print('Confusion Matrix for train & test set: \\n',\n",
    "          '\\nTrain set\\n',\n",
    "          confusion_matrix(y_train, y_pred_train), '\\n'\n",
    "          '\\n\\nTest set\\n',)\n",
    "    print(confusion_matrix(y_test, y_pred_test), '\\n')\n",
    "\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "    print('\\nClassification Report for train & test set\\n',\n",
    "          '\\nTrain set\\n',\n",
    "          classification_report(y_train, y_pred_train),\n",
    "          '\\n\\nTest set\\n',\n",
    "          classification_report(y_test, y_pred_test))\n",
    "\n",
    "    print('-----------------------------------------------------\\n')\n",
    "\n",
    "    # print('roc auc score for train and test set:\\n ',\n",
    "    #       round(roc_auc_score(y_train, y_pred_train), 4),\n",
    "    #       round(roc_auc_score(y_test, y_pred_test), 4))\n",
    "    \n",
    "    \n",
    "def plot_feature_importances(model):\n",
    "    \"\"\"\n",
    "        The function to plot the coefficients of tree based models.\n",
    "  \n",
    "        Parameters:\n",
    "            model: classifier model  \n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    #barplot\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values)\n",
    "    plt.title('Comparison of Feature Importances')\n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.948200Z",
     "start_time": "2021-04-09T04:49:41.632Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sensorless_drive_diagnosis.txt\", delim_whitespace=True, header=None)\n",
    "header_names = ['feat' + str(i) for i in range(1, df.shape[1])]\n",
    "header_names.append('class')\n",
    "df.set_axis(header_names, axis=1, inplace=True)\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing Values\n",
    "\n",
    "There are not any misssing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.950262Z",
     "start_time": "2021-04-09T04:49:41.638Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the basic info\n",
    "\n",
    "48non-null float(features), 1 non-null integer(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.951767Z",
     "start_time": "2021-04-09T04:49:41.643Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Class Sizes\n",
    "\n",
    "There is not a class imbalance issue . All the classes have the same number of samples. Because there is not a class imbalance problem, accuracy is enough as a metric. \n",
    "\n",
    "We are going to check <b>precision, recall, and F1 scores </b>, but in the grid search and random search we are going to use accuracy to evaluate model performance. \n",
    "\n",
    "We are not going to calculate <b>F-Beta score</b> and <b>Cohen's Kappa score </b>  in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.953099Z",
     "start_time": "2021-04-09T04:49:41.647Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Target and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.954491Z",
     "start_time": "2021-04-09T04:49:41.652Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a random sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.955852Z",
     "start_time": "2021-04-09T04:49:41.657Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample = df.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "Results of the histograms: There are a lot of features that have exact same distribution. There can be a high correlation between features. For example <b>feat24</b>, <b>feat25</b>, <b>feat26</b>, <b>feat27</b> are the same. \n",
    "\n",
    "A heatmap can help us see the amount of correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.957129Z",
     "start_time": "2021-04-09T04:49:41.662Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 48\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "# plt.figure(figsize=(16,48))\n",
    "X.hist(layout=(12,4))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps\n",
    "\n",
    "Heatmap with annotations show the correlated columns and the amount of correlation. \n",
    "\n",
    "#### Heatmap 1\n",
    "At the below map, <b>18 columns</b> are 100% correlated to each other. Dropping them in the beginning of the analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.958409Z",
     "start_time": "2021-04-09T04:49:41.667Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_heatmap = df_sample.drop(['class'], axis=1)\n",
    "plt.figure(figsize=(40, 40))\n",
    "ax = sns.heatmap(df_heatmap.corr(), center=0, linewidths=.5, annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Correlated Columns\n",
    "\n",
    "18 columns are dropped, 30 columns are kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.959870Z",
     "start_time": "2021-04-09T04:49:41.671Z"
    }
   },
   "outputs": [],
   "source": [
    "# 100% correlated columns are dropped\n",
    "columns_to_drop = ['feat8', 'feat9', 'feat11', 'feat12', 'feat16',\n",
    "                   'feat20','feat21','feat22','feat23','feat24', \n",
    "                   'feat32','feat33','feat35', 'feat36',\n",
    "                   'feat44','feat45','feat47','feat48']\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap 2\n",
    "\n",
    "According to the heatmap below, there are not 100% correlated columns but <b>feat13</b>, <b>feat15</b>, <b>feat17</b> are also highly correlated to <b>feat14</b>. They can affect the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.960676Z",
     "start_time": "2021-04-09T04:49:41.799Z"
    }
   },
   "outputs": [],
   "source": [
    "df_heatmap = df.drop(['class'], axis=1) # dropping target columns and making correlation heatmap of features. \n",
    "plt.figure(figsize=(40, 40)) \n",
    "ax = sns.heatmap(df_heatmap.corr(), center=0, linewidths=.5, annot=True, annot_kws={\"size\": 20})\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5) # this line is to fix a bug at the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seperating target and features after dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.963036Z",
     "start_time": "2021-04-09T04:49:41.806Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling Features To Get Better Boxplots (MinMax Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.964289Z",
     "start_time": "2021-04-09T04:49:41.810Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "# scaled_X = pd.DataFrame(scaled_x_train, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.965549Z",
     "start_time": "2021-04-09T04:49:41.813Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (35, 20)},font_scale=2.1)  # Set font scale   \n",
    "g = sns.boxplot(data=scaled_X)\n",
    "for item in g.get_xticklabels():  # Rotate x labels to 70 degrees angle\n",
    "    item.set_rotation(70)\n",
    "plt.title('Boxplots Of Features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.966962Z",
     "start_time": "2021-04-09T04:49:41.818Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = ['feat1', 'feat2', 'feat3', 'feat4', 'feat5', \n",
    "                     'feat6', 'feat7', 'feat10', 'feat13', 'feat14', \n",
    "                     'feat15', 'feat17', 'feat18', 'feat19', 'feat25']\n",
    "    \n",
    "fig, ax = plt.subplots(5, 3)\n",
    "for variable, subplot in zip(selected_features, ax.flatten()):\n",
    "    sns.lineplot(df.index, df[variable], ax=subplot, hue=df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:41.967948Z",
     "start_time": "2021-04-09T04:49:41.821Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_features = ['feat26', 'feat27', 'feat28', 'feat29', 'feat30', \n",
    "                     'feat31', 'feat34', 'feat37', 'feat38', 'feat39', \n",
    "                     'feat40', 'feat41', 'feat42', 'feat43', 'feat46']\n",
    "    \n",
    "fig, ax = plt.subplots(5, 3)\n",
    "for variable, subplot in zip(selected_features, ax.flatten()):\n",
    "    sns.lineplot(df.index, df[variable], ax=subplot, hue=df['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "\n",
    "### Models in this notebook: Logistic Regression, Random Forest Classifier, XGBoost Classifier, Multi Layer Perceptron\n",
    "\n",
    "I decided to use logistic regression as a baseline model. \n",
    "\n",
    "I chose random forest and XGBoost as main models. Because they are good at handling complex, non-linear relationships.\n",
    "\n",
    "I added a multiclass perceptron as a deep learning model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting The Data Into Test and Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:42.197668Z",
     "start_time": "2021-04-09T04:49:42.071653Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Training Data for Logistic Regression Model (Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:42.280692Z",
     "start_time": "2021-04-09T04:49:42.200281Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "scaled_X_train = pd.DataFrame(scaled_X_train, columns=X_train.columns)\n",
    "scaled_X_test = pd.DataFrame(scaled_X_test, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:42.319101Z",
     "start_time": "2021-04-09T04:49:42.283698Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_X_train_log = scaled_X_train.drop(['feat13', 'feat15', 'feat17' ], axis=1)\n",
    "scaled_X_test_log = scaled_X_test.drop(['feat13', 'feat15', 'feat17'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T04:49:42.327994Z",
     "start_time": "2021-04-09T04:49:42.320971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702, 27)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.026Z"
    }
   },
   "outputs": [],
   "source": [
    "log = LogisticRegression(solver='saga', max_iter=5000)\n",
    "log.fit(scaled_X_train_log, y_train)\n",
    "y_pred_train = log.predict(scaled_X_train_log)\n",
    "y_pred_test = log.predict(scaled_X_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.029Z"
    }
   },
   "outputs": [],
   "source": [
    "model_evaluation(scaled_X_train_log, scaled_X_test_log, y_train, y_test, y_pred_train, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.031Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_coefficients(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Logistic Regression\n",
    "\n",
    "##### Assumptions of Logistic Regression\n",
    "\n",
    "Logitic Regression has normality, linearity, and homoscedasticity assumptions. We can make log normalization to have a more normal distribution. We can check the residuals for homoscedasticty. \n",
    "\n",
    "##### Feature Selection\n",
    "\n",
    "We should get rid of outliers that have too much influence and leverage. We should eliminate the correlated columns by Variance Inflation Factor(VIF) or Predictive Power Score(PPS). We can also use Lasso and Ridge penalties for feature selection. We can apply other filter or wrapper methods. \n",
    "\n",
    "Because this is my vanilla model, I decided to continue with other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.035Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', random_state=42, n_estimators= 80)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_train = rf_clf.predict(X_train)\n",
    "y_pred_test = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.040Z"
    }
   },
   "outputs": [],
   "source": [
    "model_evaluation(X_train, X_test, y_train, y_test, y_pred_train, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.043Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Grid Search - Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.046Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('classifier', RandomForestClassifier(random_state=123))])\n",
    "# grid = [{'classifier__criterion': ['gini', 'entropy'],\n",
    "#          'classifier__n_estimators':[100, 120, 150],\n",
    "#          'classifier__max_depth': [12, 14],\n",
    "#          'classifier__min_samples_split': [4, 6, 8]}]\n",
    "# clf = GridSearchCV(estimator=pipe, param_grid=grid,\n",
    "#                    cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "# print(\"Grid search..\")\n",
    "# search_time_start = time.time()\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_train = clf.predict(X_train)\n",
    "# y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# search_time_end = time.time()\n",
    "# print('Time for grid search in seconds', search_time_end - search_time_start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.049Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameter Combination Found During Grid Search:\\n \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Results \n",
    "\n",
    "Below are the best parameters of above grid search. I commented out the cell, it takes 22 minutes time to run the grid search on my MacBook Pro \n",
    "       \n",
    "       \n",
    "  \n",
    "  Processor 2.2 GHz Intel Core i7,     Memory: 16 GB 1600 MHz DDR3     Operation System: macOS Mojave. \n",
    "  \n",
    " You can just uncomment and run the above cell if needed. \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      'classifier__criterion': 'entropy'\n",
    "      'classifier__max_depth': 14\n",
    "      'classifier__min_samples_split': 4\n",
    "      'classifier__n_estimators': 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.052Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', random_state=42, n_estimators= 150,\n",
    "                             criterion = 'entropy', max_depth = 14, min_samples_split = 4)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_train = rf_clf.predict(X_train)\n",
    "y_pred_test = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.054Z"
    }
   },
   "outputs": [],
   "source": [
    "model_evaluation(X_train, X_test, y_train, y_test, y_pred_train, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.057Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search - XGBoost Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran a lot of different models and grid searches before starting this random search. I didn't include them all. We can find the best combination after many iterations depending on the computational power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.061Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(objective=\"multi:softmax\")\n",
    "param_grid = {\n",
    "        'silent': [False],\n",
    "        'max_depth': [6, 10, 15, 20],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'n_estimators': [100]}\n",
    "RS_clf = RandomizedSearchCV(estimator=clf, param_distributions=param_grid,\n",
    "                            cv=5, n_jobs=-1)\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "\n",
    "RS_clf.fit(X_train, y_train)\n",
    "y_pred_train = RS_clf.predict(X_train)\n",
    "y_pred_test = RS_clf.predict(X_test)\n",
    "\n",
    "search_time_end = time.time()\n",
    "print('Time for randomized search in minutes: ', round((search_time_end - search_time_start)/60 ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.064Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameter Combination Found During Random Search:\\n \", RS_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the best parameters of above random search. I commented out the cell, it takes 38 minutes to run the grid search on my MacBook Pro \n",
    "       \n",
    "       \n",
    "  \n",
    "  Processor 2.2 GHz Intel Core i7,     Memory: 16 GB 1600 MHz DDR3     Operation System: macOS Mojave. \n",
    "  \n",
    " You can just uncomment and run the above cell if needed. \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    'subsample': 0.9, \n",
    "\n",
    "    'reg_lambda': 0.1, \n",
    "\n",
    "    'n_estimators': 100, \n",
    "\n",
    "    'min_child_weight': 1.0, \n",
    "\n",
    "    'max_depth': 10, \n",
    "\n",
    "    'learning_rate': 0.1, \n",
    "\n",
    "    'gamma': 0.5, \n",
    "\n",
    "    'colsample_bytree': 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final XGBoost Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.146Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(objective='multi:softmax', nthread=4, scale_pos_weight=3,\n",
    "                        colsample_bytree= 0.7, gamma= 0.5, learning_rate= 0.1, \n",
    "                        max_depth= 10,min_child_weight= 1.0, reg_lambda= 1.0, \n",
    "                        silent= False, subsample= 0.9, seed=42, n_estimators=100)\n",
    "search_time_start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "search_time_end = time.time()\n",
    "print('Time for the final XG Boost model in seconds: ', round(search_time_end - search_time_start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.151Z"
    }
   },
   "outputs": [],
   "source": [
    "model_evaluation(X_train, X_test, y_train, y_test, y_pred_train, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.155Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.160Z"
    }
   },
   "outputs": [],
   "source": [
    "search_time_start = time.time()\n",
    "\n",
    "print('Multi Layer Perceptron')\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,128,128,128,128,128), activation='relu', \n",
    "                    solver='adam', max_iter=200, verbose=True)\n",
    "mlp.fit(X_train,y_train)\n",
    "Y_train_mlp_pred_df = mlp.predict(X_train)\n",
    "Y_valid_mlp_pred_df = mlp.predict(X_test)\n",
    "\n",
    "search_time_end = time.time()\n",
    "print('Time for the final MLP model in seconds: ', search_time_end - search_time_start )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.167Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_mlp_train = confusion_matrix(y_train, Y_train_mlp_pred_df)\n",
    "print(cm_mlp_train)\n",
    "mlp_train_accuracy = cm_mlp_train.trace()/cm_mlp_train.sum()\n",
    "\n",
    "print('MLP accuracy on a train set is: ', round(mlp_train_accuracy, 6))\n",
    "print(60*'-')\n",
    "\n",
    "cm_mlp = confusion_matrix(y_test, Y_valid_mlp_pred_df)\n",
    "print(cm_mlp)\n",
    "mlp_test_accuracy = cm_mlp.trace()/cm_mlp.sum()\n",
    "\n",
    "print('MLP accuracy on a test set is: ', round(mlp_test_accuracy, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T04:49:42.183Z"
    }
   },
   "outputs": [],
   "source": [
    "def baseline_model(input_dim,output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(input_dim*2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(input_dim*2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(input_dim*2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(input_dim*2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(input_dim//2, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
